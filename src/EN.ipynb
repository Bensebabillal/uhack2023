{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "This Jupyter notebook is designed to help you understand the fundamental concepts of data engineering and apply them in practice by building a dataset that you can use for your hackathon project.\n",
    "\n",
    "Data engineering is the process of designing, building, testing, and maintaining the systems and infrastructure required to collect, store, process, and analyze large volumes of data. It involves a range of skills and technologies, including database design, data modeling, data integration, data processing, data warehousing, and data visualization.\n",
    "\n",
    "In this workshop, we will cover some of the key concepts of data engineering at a basic level, including data collection, data cleaning, data integration, data storage, data processing and data visualization.\n",
    "\n",
    "Data engineering is getting more and more important in software development with applications and SaaS products becoming more and more data-driven. Developing solutions with a data first mindset is a key skill for any software engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Analysis of the provided data\n",
    "\n",
    "To begin the process, we first analyze the data provided by the client, which is in the form of a CSV file. This analysis involves understanding the different data types present and the overall distribution of the data. Additionally, we also examine the quality of the data, which includes assessing its completeness and consistency with the business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read work order template provided - datasets/ville_gatineau_data.xlsx\n",
    "work_order_template = pd.read_excel('/home/jovyan/work/datasets/ville_gatineau_data.xlsx', sheet_name='Bon de travail1')\n",
    "\n",
    "# Read delivery order template provided - datasets/ville_gatineau_data.xlsx\n",
    "delivery_order_template = pd.read_excel('/home/jovyan/work/datasets/ville_gatineau_data.xlsx', sheet_name='Feuille de livraison1')\n",
    "\n",
    "work_order_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Results\n",
    "\n",
    "Upon examining the data, we have observed that it is partially complete and inconsistent. However, before we can effectively use this data for analyses and to develop the features needed for the hackathon, there are a few issues that must be addressed.\n",
    "\n",
    "1. The addresses provided in the dataset are not real addresses\n",
    "2. The addresses lack geolocation information (latitude, longitude, geometry) for certain itineraries use cases\n",
    "3. Several columns contain missing values (NaN)\n",
    "4. There is unnamed columns at the end of the dataset that contains no data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data collection\n",
    "Instead of relying on the provided data, we will collect our own data from public sources.\n",
    "\n",
    "Data collection involves gathering data from various sources such as databases, APIs, web scraping, or user input. The data can be structured or unstructured, and may be stored in various formats such as CSV, JSON, or XML. Generally, data collection is automated using scripts or programs that can be run periodically to collect new data.\n",
    "\n",
    "In the upcoming step, we aim to source data from public sources to address the previously identified issues. Acquiring high-quality data will help us in developing better features and conducting thorough analyses. For this purpose, we will obtain data from the Open Data section of the Ville de Gatineau website.\n",
    "\n",
    "1. Addresses - Public locations within the city of Gatineau\n",
    "2. Sectors - Administrative divisions of the city of Gatineau\n",
    "3. Districts - Administrative divisions of the city of Gatineau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read lieux publics data from Gatineau open data portal without downloading\n",
    "# https://www.gatineau.ca/upload/donneesouvertes/LIEU_PUBLIC.csv\n",
    "addresses = pd.read_csv('https://www.gatineau.ca/upload/donneesouvertes/LIEU_PUBLIC.csv', sep=',')\n",
    "\n",
    "# Read sectors data from Gatineau open data portal without downloading\n",
    "# https://www.gatineau.ca/upload/donneesouvertes/LIEU_PUBLIC.csv\n",
    "sectors = pd.read_csv('https://www.gatineau.ca/upload/donneesouvertes/DECOUPAGE_ADMINISTRATIF.csv', sep=',')\n",
    "\n",
    "# Read districts data from Gatineau open data portal without downloading\n",
    "# https://www.gatineau.ca/upload/donneesouvertes/DISTRICT_ELECTORAUX_A_VENIR_2021.csv\n",
    "districts = pd.read_csv('https://www.gatineau.ca/upload/donneesouvertes/DISTRICT_ELECTORAUX_A_VENIR_2021.csv', sep=',')\n",
    "\n",
    "# Read mock customer data from open data portals\n",
    "customers1 = pd.read_csv('/home/jovyan/work/datasets/mock_customers1.csv', sep=',')\n",
    "customers2 = pd.read_csv('/home/jovyan/work/datasets/mock_customers2.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers = pd.concat([customers1, customers2], ignore_index=True)\n",
    "customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Data cleaning\n",
    "Now that we have real data to work with, we can begin the process of cleaning the data.\n",
    "\n",
    "Data cleaning involves removing invalid or irrelevant data, handling missing values, formatting data, and ensuring data accuracy. Some of the techniques used in data cleaning include data profiling, data validation, and data standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addresses - Selecting & Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Select the relevant columns and rename them to match the work order template\n",
    "addresses = addresses[['CODEID', 'TYPE', 'ADR_COMPLE', 'GEOM']]\n",
    "addresses.columns = ['ADDRESS_ID', 'TYPE_NAME', 'FULL_ADDRESS', 'GEOM']\n",
    "\n",
    "# Execute ydata_profiling to gather intelligence about the data\n",
    "profile = ProfileReport(addresses, title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addresses - Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with null values, empty strings, and non-numeric values\n",
    "addresses = addresses[\n",
    "    addresses['FULL_ADDRESS'].notnull() &\n",
    "    addresses['FULL_ADDRESS'].str.match(r'^\\d+') &\n",
    "    addresses['FULL_ADDRESS'].str.strip() != ''\n",
    "]\n",
    "\n",
    "# Remove rows with null values, empty strings, and non-numeric values\n",
    "addresses = addresses[addresses['FULL_ADDRESS'].notna()]\n",
    "\n",
    "# Identify non-unique/duplicate rows based on FULL_ADDRESS column\n",
    "dups = addresses[addresses.duplicated(subset=['FULL_ADDRESS'], keep=False)].sort_values(by=['FULL_ADDRESS'])\n",
    "\n",
    "if len(dups):\n",
    "    print(f'Non-unique addresses identified: {len(dups)}')\n",
    "\n",
    "# Remove non-unique/duplicate addresses\n",
    "addresses = addresses.drop_duplicates(subset=['FULL_ADDRESS'], keep='first')\n",
    "\n",
    "# Execute ydata_profiling to gather intelligence about the cleaned data\n",
    "profile = ProfileReport(addresses, title=\"Addresses Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customers - Selecting & Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Select the relevant columns and rename them to match the work order template\n",
    "customers = customers[['first_name', 'last_name', 'email', 'phonenumber']]\n",
    "customers.columns = ['FIRSTNAME', 'LASTNAME', 'EMAIL', 'PHONENUMBER']\n",
    "\n",
    "# Execute ydata_profiling to gather intelligence about the data\n",
    "profile = ProfileReport(customers, title=\"Customers Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sectors - Selecting & Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the relevant columns and rename them to match the work order template\n",
    "sectors = sectors[['CODEID', 'NOM', 'GEOM']]\n",
    "sectors.columns = ['SECTOR_ID', 'SECTOR_NAME', 'GEOM']\n",
    "\n",
    "# Identify non-unique/duplicate rows based on SECTOR_NAME column\n",
    "dups = sectors[sectors.duplicated(subset=['SECTOR_NAME'], keep=False)].sort_values(by=['SECTOR_NAME'])\n",
    "if len(dups):\n",
    "    print(f'Non-unique sectors identified: {len(dups)}')\n",
    "\n",
    "# Remove non-unique/duplicate sectors\n",
    "sectors = sectors.drop_duplicates(subset=['SECTOR_NAME'], keep='first')\n",
    "\n",
    "# Execute ydata_profiling to gather intelligence about the cleaned data\n",
    "profile = ProfileReport(sectors.drop(columns=['GEOM']), title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Districts - Selecting & Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Districts - select the columns we need and rename to readable names\n",
    "districts = districts[['CODEID', 'DISTRICT', 'GEOM']]\n",
    "districts.columns = ['DISTRICT_ID', 'DISTRICT_NAME', 'GEOM']\n",
    "\n",
    "# Identify non-unique/duplicate districts based on DISTRICT_NAME column\n",
    "dups = districts[districts.duplicated(subset=['DISTRICT_NAME'], keep=False)].sort_values(by=['DISTRICT_NAME'])\n",
    "if len(dups):\n",
    "    print(f'Non-unique districts identified: {len(dups)}')\n",
    "\n",
    "# Remove non-unique/duplicate districts\n",
    "districts = districts.drop_duplicates(subset=['DISTRICT_NAME'], keep='first')\n",
    "\n",
    "profile = ProfileReport(districts.drop(columns=['GEOM']), title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting to Geopandas**\n",
    "\n",
    "In this step, we convert the dataframes to geopandas dataframes. This will allow us to perform spatial operations on the dataframes. \n",
    "\n",
    "However, when attempting to convert the dataframes, we encounter errors. This is because the dataframes contain invalid geometries. To fix this issue, we need to convert the geometries to the correct format.\n",
    "\n",
    "We use a simple string replace to fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "# Create a geopandas dataframe from the addresses dataframe\n",
    "addresses_gdf = gpd.GeoDataFrame(addresses, geometry=gpd.GeoSeries(addresses['GEOM'].apply(wkt.loads)))\n",
    "addresses_gdf = addresses_gdf.drop(columns=['GEOM'])\n",
    "\n",
    "# Create a geopandas dataframe from the sectors dataframe and convert the GEOM column to a valid POLYGON\n",
    "sectors['GEOM'] = sectors['GEOM'].apply(lambda x: x.replace('POLYGON (', 'POLYGON ((').replace(')', '))'))\n",
    "sectors_gdf = gpd.GeoDataFrame(sectors, geometry=gpd.GeoSeries(sectors['GEOM'].apply(wkt.loads)))\n",
    "sectors_gdf = sectors_gdf.drop(columns=['GEOM'])\n",
    "\n",
    "# Create a sectors dataframe without the GEOM column for application use\n",
    "sectors = sectors.drop(columns=['GEOM', 'geometry'])\n",
    "\n",
    "# Create a geopandas dataframe from the sectors dataframe\n",
    "districts_gdf = gpd.GeoDataFrame(districts, geometry=gpd.GeoSeries(districts['GEOM'].apply(wkt.loads)))\n",
    "districts_gdf = districts_gdf.drop(columns=['GEOM'])\n",
    "\n",
    "# Create a districts dataframe without the GEOM column for application use\n",
    "districts = districts.drop(columns=['GEOM', 'geometry'])\n",
    "\n",
    "print('Geopandas conversion completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Data integration\n",
    "\n",
    "Data integration involves combining data from multiple sources and integrating them into a single dataset. This is typically done using tools such as ETL (extract, transform, load) or ELT (extract, load, transform) processes. Data integration is a critical step in the data engineering process as it allows us to combine data from multiple sources into a single dataset that can be used for analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a spatial join between the addresses and districts dataframes based on the geometry columns\n",
    "join1 = gpd.sjoin(addresses_gdf, districts_gdf, how='left', predicate='within')\n",
    "join1 = join1[['ADDRESS_ID', 'FULL_ADDRESS', 'DISTRICT_NAME', 'geometry']]\n",
    "\n",
    "# Perform a spatial join between addresses and sectors dataframes based on the geometry columns\n",
    "join2 = gpd.sjoin(addresses_gdf, sectors_gdf, how='left', predicate='within')\n",
    "join2 = join2[['ADDRESS_ID', 'FULL_ADDRESS', 'SECTOR_NAME', 'geometry']]\n",
    "\n",
    "# Merge the join1 and join2 dataframes based on the geometry column\n",
    "joined = addresses_gdf.merge(join1.merge(join2, on='geometry'), on='geometry')\n",
    "joined = joined[['ADDRESS_ID', 'FULL_ADDRESS', 'SECTOR_NAME', 'DISTRICT_NAME', 'geometry']]\n",
    "\n",
    "# Execute ydata_profiling to gather intelligence about the cleaned data\n",
    "profile = ProfileReport(joined.drop(columns=['geometry']), title=\"Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Creating a mock dataset\n",
    "Data transformation involves converting the data into a format suitable for analysis (Dimensions and Facts), including normalization, aggregation, or discretization. Some common data transformation techniques include pivot tables, joins, and aggregations. Generally, performant tools like Spark and dbt will allow transformation at scale.\n",
    "\n",
    "Generally, transformations are performed on a dataset that is stored in a data warehouse. However, for the purposes of this workshop, we will perform the transformations using our pandas dataframes.\n",
    "\n",
    "In this step, we mimic the process of data transformation by creating a mock dataset. This dataset will be used for developing the features needed for the hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# List the supported bins for the application (coming from the work order template)\n",
    "supported_bins = [\n",
    "    'BAC BLEU - 360',\n",
    "    'BAC BRUN - 45',\n",
    "    'BAC BRUN - 80',\n",
    "    'BAC BRUN - 120',\n",
    "    'BAC BRUN - 240',\n",
    "    'BAC GRIS - 120',\n",
    "    'BAC GRIS - 240',\n",
    "    'BAC GRIS - 360',\n",
    "]\n",
    "\n",
    "# Create a copy of geopandas dataframe work_order_template and remove all rows\n",
    "orders = work_order_template.copy()\n",
    "orders = orders.iloc[0:0]\n",
    "\n",
    "# Remove unwanted columns Unnamed: 36,Unnamed: 37\n",
    "orders = orders.drop(columns=['Unnamed: 36', 'Unnamed: 37'])\n",
    "\n",
    "# Create work orders for each addresses in the joined dataframe\n",
    "orders['ID']                    = joined['FULL_ADDRESS'].apply(lambda x: random.randint(5330000000, 5520000000))\n",
    "orders['STREET_NUMBER']         = joined['FULL_ADDRESS'].str.extract(r'^(\\d+)')\n",
    "orders['STREET']                = joined['FULL_ADDRESS'].str.extract(r'^\\d+\\s(.*)')\n",
    "orders['SECTOR']                = joined['SECTOR_NAME']\n",
    "orders['CITY']                  = 'Gatineau'\n",
    "orders['DISTRICT']              = joined['DISTRICT_NAME']\n",
    "orders['NOTE']                  = orders['NOTE'].apply(lambda x: ['RUE', 'COUR'][random.randint(0, 1)])\n",
    "orders['CLIENT_TYPE']           = 'AUTRES'\n",
    "orders['BILLING_TYPE']          = orders['BILLING_TYPE'].apply(lambda x: ['RESIDENTIEL', 'MUNICIPAL'][random.randint(0, 1)])\n",
    "orders['REQUEST_NUMBER']        = orders['REQUEST_NUMBER'].apply(lambda x: f'F{random.randint(400000, 500000)}')\n",
    "orders['REQUEST']               = 'Réparation'\n",
    "orders['BIN']                   = orders['BIN'].apply(lambda x: supported_bins[random.randint(0, 1)])\n",
    "orders['PART']                  = orders['PART'].apply(lambda x: ['BODY', 'COUVERCLE', 'ROUE'][random.randint(0, 2)])\n",
    "orders['STATUS']                = orders['PART'].apply(lambda x: ['En attente', 'En progrès', 'Complété'][random.randint(0, 2)])\n",
    "orders['EXCEPTION']             = None\n",
    "orders['NOTE_EXCEPTION']        = None\n",
    "orders['STATUS_QUARANTAINE']    = None\n",
    "orders['geometry']              = joined['geometry']\n",
    "\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we created mock values for the rows coming from the addresses dataset. The values are based on the data in the work orders template.\n",
    "\n",
    "In the next section, we apply some business logic to mock the dates and times of the work orders while having an interesting distribution of data. This is done by randomly selecting a date and time from the provided ranges following the next rules:\n",
    "\n",
    "1. A work order that is completed must have been opened between 90 and 240 days before today\n",
    "2. A work order that is completed must have been completed between 14 and 45 days after its opening date\n",
    "3. A work order that is 'In progress' must have been opened between 0 and 60 days before today\n",
    "4. A work order that is 'In waiting' must have been opened between 0 and 60 days before today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define date ranges for each status\n",
    "date_ranges = {'En attente': 60, 'En progrès': 60, 'Complété': 240}\n",
    "last_3_months_range = 90  # days\n",
    "\n",
    "# Define current date and date format\n",
    "now = datetime.now()\n",
    "date_format = '%m/%d/%Y'\n",
    "\n",
    "# Define a function to generate random dates within a given range\n",
    "def random_order_date(status):\n",
    "    range_days = date_ranges[status]\n",
    "    return (now - timedelta(days=np.random.randint(0 if status != 'Complété' else 60, range_days))).strftime(date_format)\n",
    "\n",
    "def random_closed_date(order_date):\n",
    "    return (datetime.strptime(order_date, date_format) + timedelta(days=np.random.randint(14, 45))).strftime(date_format)\n",
    "\n",
    "# Generate ORDER_DATE and ORDER_TIME columns\n",
    "orders['ORDER_DATE'] = [random_order_date(status) for status in orders['STATUS']]\n",
    "orders['ORDER_TIME'] = '12:00:00 AM'\n",
    "\n",
    "# Generate CLOSED_DATE and CLOSED_TIME columns\n",
    "orders['CLOSED_DATE'] = orders.apply(lambda x: random_closed_date(x['ORDER_DATE']) if x['STATUS'] == 'Complété' else None, axis=1)\n",
    "orders['CLOSED_TIME'] = orders['STATUS'].apply(lambda x: '12:00:00 AM' if x == 'Complété' else None)\n",
    "\n",
    "# Create a GeoDataFrame and drop the geometry column from the original DataFrame\n",
    "orders_gdf = gpd.GeoDataFrame(orders, geometry=gpd.GeoSeries(orders['geometry']))\n",
    "orders.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Data storage\n",
    "\n",
    "Data storage involves storing the data in a database, data warehouse, or data lake. Some popular databases include MySQL, PostgreSQL, and MongoDB, while popular data warehouses include Amazon Redshift and Google BigQuery.\n",
    "\n",
    "A new type of solution that is gaining massive popularity is the data cloud. Data clouds are data warehouses that are hosted on the cloud and are accessible via a REST API. Some popular data clouds include Snowflake and Databricks.\n",
    "\n",
    "Without having to dive to deep in analytical database modeling, we will use the dataframe structure to store data in a database and in an exportable data format (CSV).\n",
    "\n",
    "We will use two types of tables:\n",
    "\n",
    "1. Raw - The raw data is stored as is in the database (from the dataframes)\n",
    "2. Modeled - The data is transformed and stored in a database with proper data types and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create CSV files for each dataframe\n",
    "orders.to_csv('/home/jovyan/work/orders.csv', index=False)\n",
    "customers.to_csv('/home/jovyan/work/customers.csv', index=False)\n",
    "districts.to_csv('/home/jovyan/work/districts.csv', index=False)\n",
    "sectors.to_csv('/home/jovyan/work/sectors.csv', index=False)\n",
    "print('Dataframes written to CSV files')\n",
    "\n",
    "# Create a connection to the postgres database\n",
    "engine = create_engine('postgresql://postgres:postgres@postgres:5432/postgres')\n",
    "\n",
    "# Write the dataframes to the database\n",
    "orders.to_sql('orders_raw', engine, if_exists='replace', index=False)\n",
    "customers.to_sql('customers_raw', engine, if_exists='replace', index=False)\n",
    "districts.to_sql('districts_raw', engine, if_exists='replace', index=False)\n",
    "sectors.to_sql('sectors_raw', engine, if_exists='replace', index=False)\n",
    "print('Dataframes written to database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a write connection\n",
    "connection = psycopg2.connect(\n",
    "    host=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    database=\"postgres\"\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "print('Connection to database established')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple SQL transformation to create the orders table\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS sectors;\n",
    "    CREATE TABLE sectors AS\n",
    "    SELECT\n",
    "        cast(\"SECTOR_ID\" as integer) as sector_id,\n",
    "        \"SECTOR_NAME\" as sector_name\n",
    "    FROM sectors_raw;\n",
    "'''\n",
    "\n",
    "# Execute the SQL transformation and commit the changes to the database\n",
    "cursor.execute(sql)\n",
    "connection.commit()\n",
    "print('SQL transformation executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple SQL transformation to create the orders table\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS districts;\n",
    "    CREATE TABLE districts AS\n",
    "    SELECT\n",
    "        cast(\"DISTRICT_ID\" as integer) as district_id,\n",
    "        \"DISTRICT_NAME\" as district_name\n",
    "    FROM districts_raw;\n",
    "'''\n",
    "\n",
    "# Execute the SQL transformation and commit the changes to the database\n",
    "cursor.execute(sql)\n",
    "connection.commit()\n",
    "print('SQL transformation executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple SQL transformation to create the orders table\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS customers;\n",
    "    CREATE TABLE customers AS\n",
    "    SELECT\n",
    "        cast(\"FIRSTNAME\" as varchar) as firstname,\n",
    "        cast(\"LASTNAME\" as varchar) as lastname,\n",
    "        cast(\"EMAIL\" as varchar) as email,\n",
    "        cast(\"PHONENUMBER\" as varchar) as phone_number\n",
    "    FROM customers_raw;\n",
    "'''\n",
    "\n",
    "# Execute the SQL transformation and commit the changes to the database\n",
    "cursor.execute(sql)\n",
    "connection.commit()\n",
    "print('SQL transformation executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple SQL transformation to create the orders table\n",
    "sql = '''\n",
    "    DROP VIEW IF EXISTS delivery_summary;\n",
    "    DROP TABLE IF EXISTS orders;\n",
    "    CREATE TABLE orders AS\n",
    "    SELECT\n",
    "        cast(\"ID\" as bigint) as order_id,\n",
    "        cast(\"STREET_NUMBER\" as integer) as street_number,\n",
    "        cast(\"STREET\" as varchar) as street,\n",
    "        cast(\"SECTOR\" as varchar) as sector,\n",
    "        cast(\"CITY\" as varchar) as city,\n",
    "        cast(\"DISTRICT\" as varchar) as district,\n",
    "        cast(\"NOTE\" as varchar) as note,\n",
    "        cast(\"CLIENT_TYPE\" as varchar) as client_type,\n",
    "        cast(\"BILLING_TYPE\" as varchar) as billing_type,\n",
    "        cast(\"REQUEST_NUMBER\" as varchar) as request_number,\n",
    "        cast(\"REQUEST\" as varchar) as request,\n",
    "        cast(\"BIN\" as varchar) as bin,\n",
    "        cast(\"PART\" as varchar) as part,\n",
    "        cast(\"STATUS\" as varchar) as status,\n",
    "        cast(\"EXCEPTION\" as varchar) as exception,\n",
    "        cast(\"NOTE_EXCEPTION\" as varchar) as note_exception,\n",
    "        cast(\"STATUS_QUARANTAINE\" as varchar) as status_quarantine,\n",
    "        cast(\"ORDER_DATE\" as date) as order_date,\n",
    "        cast(\"ORDER_TIME\" as time) as order_time,\n",
    "        cast(\"CLOSED_DATE\" as date) as closed_date,\n",
    "        cast(\"CLOSED_TIME\" as time) as closed_time\n",
    "    FROM orders_raw;\n",
    "'''\n",
    "\n",
    "# Execute the SQL transformation and commit the changes to the database\n",
    "cursor.execute(sql)\n",
    "connection.commit()\n",
    "print('SQL transformation executed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mock the delivery template provided, we will create a database view that will allow us to query the data in the same way as the template.\n",
    "\n",
    "Views are a powerful tool that allow us to create a virtual table that is based on the result of a query. This allows us to create a table that is based on the result of a query without having to store the data in a table.\n",
    "\n",
    "When data is changed in the source tables, the view will automatically reflect the changes.\n",
    "\n",
    "This is a great feature for analytical purposes like reporting, dashboards and summarizing data from other tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple SQL transformation to create the orders table\n",
    "sql = \"\"\"\n",
    "    DROP VIEW IF EXISTS delivery_summary;\n",
    "    CREATE VIEW delivery_summary AS\n",
    "    SELECT \n",
    "        district,\n",
    "        street_number,\n",
    "        street,\n",
    "        part,\n",
    "        SUM(CASE WHEN bin = 'BAC BLEU - 360' THEN 1 ELSE 0 END) AS \"BAC BLEU - 360\",\n",
    "        SUM(CASE WHEN bin = 'BAC BRUN - 45' THEN 1 ELSE 0 END) AS \"BAC BRUN - 45\",\n",
    "        SUM(CASE WHEN bin = 'BAC BRUN - 80' THEN 1 ELSE 0 END) AS \"BAC BRUN - 80\",\n",
    "        SUM(CASE WHEN bin = 'BAC BRUN - 120' THEN 1 ELSE 0 END) AS \"BAC BRUN - 120\",\n",
    "        SUM(CASE WHEN bin = 'BAC BRUN - 240' THEN 1 ELSE 0 END) AS \"BAC BRUN - 240\",\n",
    "        SUM(CASE WHEN bin = 'BAC GRIS - 120' THEN 1 ELSE 0 END) AS \"BAC GRIS - 120\",\n",
    "        SUM(CASE WHEN bin = 'BAC GRIS - 240' THEN 1 ELSE 0 END) AS \"BAC GRIS - 240\",\n",
    "        SUM(CASE WHEN bin = 'BAC GRIS - 360' THEN 1 ELSE 0 END) AS \"BAC GRIS - 360\"\n",
    "    FROM orders\n",
    "    WHERE status = 'En attente'\n",
    "    GROUP BY district, street_number, street, part;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL transformation and commit the changes to the database\n",
    "cursor.execute(sql)\n",
    "connection.commit()\n",
    "\n",
    "connection.close()\n",
    "print('SQL statement executed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Data processing & analysis\n",
    "\n",
    "Data processing involves performing data analysis operations such as filtering, sorting, and calculating descriptive statistics. This is typically done using programming languages such as Python or R, or using specialized tools such as Apache Spark. Data visualization involves creating charts, graphs, or other visualizations to communicate the insights obtained from the data. Some popular data visualization tools include Tableau, PowerBI, and ggplot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folium - Visualizing the distribution of data on a map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a folium map with the sectors using the mean of the coordinates as the center\n",
    "map = folium.Map(location=[joined.geometry.centroid.y.mean(), joined.geometry.centroid.x.mean()], zoom_start=10, tiles='cartodbpositron')\n",
    "\n",
    "# Add districts to the map including a tooltip\n",
    "tooltip = folium.features.GeoJsonTooltip(fields=['DISTRICT_NAME'], aliases=['District: '])\n",
    "folium.GeoJson(districts_gdf[['geometry', 'DISTRICT_NAME']].to_json(), tooltip=tooltip, name='geojson').add_to(map)\n",
    "\n",
    "# Add districts to the map including a tooltip\n",
    "addresses_tooltip = folium.features.GeoJsonTooltip(fields=['SECTOR_NAME', 'DISTRICT_NAME', 'FULL_ADDRESS'], aliases=['Sector: ', 'District: ', 'Address: '])\n",
    "folium.GeoJson(joined[['geometry', 'SECTOR_NAME', 'DISTRICT_NAME', 'FULL_ADDRESS']].to_json(), tooltip=addresses_tooltip, name='geojson').add_to(map)\n",
    "\n",
    "\n",
    "# Add a marker to the map at the coordinates of the Ville de Gatineau building\n",
    "folium.Marker([45.4684952,-75.7634862], popup='Marker', icon=folium.Icon(color='red', icon='info-sign')).add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AgglomerativeClustering - Hierarchical clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "n_clusters = 20\n",
    "\n",
    "# Calculate the pairwise distances between the data points\n",
    "orders_gdf['Latitude'] = orders_gdf['geometry'].y\n",
    "orders_gdf['Longitude'] = orders_gdf['geometry'].x\n",
    "\n",
    "distances = squareform(pdist(orders_gdf[['Latitude', 'Longitude']]))\n",
    " \n",
    "# Perform hierarchical clustering\n",
    "model = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage='complete')\n",
    "labels = model.fit_predict(distances)\n",
    "\n",
    "orders_gdf['AC_Cluster'] = labels\n",
    "\n",
    "# Create a polygon for each cluster\n",
    "cluster_polygons = []\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_polygons.append(orders_gdf[orders_gdf['AC_Cluster'] == cluster].unary_union.convex_hull)\n",
    "\n",
    "# Create a GeoDataFrame from the list of polygons and keep AC_Cluster property\n",
    "ac_clusters_gdf = gpd.GeoDataFrame({'AC_Cluster': range(n_clusters)}, geometry=cluster_polygons)\n",
    "\n",
    "# Scatter plot of the clusters\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(orders_gdf['Longitude'], orders_gdf['Latitude'], c=orders_gdf['AC_Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN - Density-based clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Define the eps and min_samples parameters\n",
    "eps = 0.005\n",
    "min_samples = 5\n",
    "\n",
    "# Create a latitude and longitude for joined\n",
    "orders_gdf['Latitude'] = orders_gdf['geometry'].y\n",
    "orders_gdf['Longitude'] = orders_gdf['geometry'].x\n",
    "\n",
    "# Convert the geodataframe to a numpy array of coordinates\n",
    "coords = orders_gdf[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# Apply DBSCAN clustering to the coordinates\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(coords)\n",
    "\n",
    "# Add the cluster labels to the geodataframe\n",
    "orders_gdf['DB_Cluster'] = dbscan.labels_\n",
    "\n",
    "# Plot all the points with a different color for each cluster where the cluster is not -1 \n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(orders_gdf['Longitude'], orders_gdf['Latitude'], c=orders_gdf['DB_Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KMeans - Centroid-based clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Number of clusters\n",
    "k = 20\n",
    "\n",
    "# Perform KMeans clustering on the location data\n",
    "orders_gdf['Latitude'] = orders_gdf['geometry'].y\n",
    "orders_gdf['Longitude'] = orders_gdf['geometry'].x\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(orders_gdf[['Latitude', 'Longitude']])\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "orders_gdf['K_Cluster'] = kmeans.labels_\n",
    "\n",
    "# Create a GeoDataFrame from the dataframe with location data\n",
    "gdf = gpd.GeoDataFrame(orders_gdf, geometry=gpd.points_from_xy(orders_gdf.Longitude, orders_gdf.Latitude))\n",
    "\n",
    "# Group the GeoDataFrame by the cluster labels and create polygons around the clusters using convex hull method\n",
    "polygons = gdf.groupby('K_Cluster')['geometry'].apply(lambda x: Polygon(x.unary_union.convex_hull.exterior.coords))\n",
    "\n",
    "# Create a GeoDataFrame from the polygons\n",
    "poly_gdf = gpd.GeoDataFrame(polygons, geometry='geometry')\n",
    "\n",
    "# Plot the polygons with matplotlib\n",
    "%matplotlib inline\n",
    "poly_gdf.plot(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation - Average wait time per cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the average time between ORDER_DATE and CLOSED_DATE for each cluster\n",
    "completion_gdf = orders_gdf[['ORDER_DATE', 'CLOSED_DATE', 'AC_Cluster']]\n",
    "\n",
    "# Filter out the rows where the CLOSED_DATE is null\n",
    "completion_gdf = completion_gdf[completion_gdf['CLOSED_DATE'].notnull()]\n",
    "\n",
    "completion_gdf['ORDER_DATE'] = pd.to_datetime(completion_gdf['ORDER_DATE'])\n",
    "completion_gdf['CLOSED_DATE'] = pd.to_datetime(completion_gdf['CLOSED_DATE'])\n",
    "\n",
    "completion_gdf['TIME_TO_COMPLETE'] = completion_gdf['CLOSED_DATE'] - completion_gdf['ORDER_DATE']\n",
    "completion_gdf['TIME_TO_COMPLETE'] = completion_gdf['TIME_TO_COMPLETE'].dt.days\n",
    "\n",
    "# Calculate the average time to complete for each cluster and create a gdf of cluster_label and avg_time_to_complete\n",
    "avg_time_to_complete = round(completion_gdf.groupby('AC_Cluster')['TIME_TO_COMPLETE'].mean())\n",
    "avg_time_to_complete_gdf = gpd.GeoDataFrame({'AC_Cluster': avg_time_to_complete.index, 'AVG_TIME_TO_COMPLETE': avg_time_to_complete.values})\n",
    "\n",
    "# Merge the average time to complete gdf with the cluster polygons gdf\n",
    "time_estimate_polygons = ac_clusters_gdf.merge(avg_time_to_complete_gdf, on='AC_Cluster')\n",
    "\n",
    "# Plot the polygons on a map using GeoPandas\n",
    "%matplotlib inline\n",
    "ax = time_estimate_polygons.plot(column='AVG_TIME_TO_COMPLETE', cmap='OrRd', alpha=0.5, edgecolor='black')\n",
    "gdf.plot(ax=ax, markersize=10, color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization - Average wait time per cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a folium map with the sectors using the mean of the coordinates as the center\n",
    "# HINT - the coordinates are in the GEOM column\n",
    "map = folium.Map(location=[time_estimate_polygons.geometry.centroid.y.mean(), time_estimate_polygons.geometry.centroid.x.mean()], zoom_start=10, tiles='cartodbpositron')\n",
    "\n",
    "# # Add the districts to the map using WKT\n",
    "tooltip = folium.features.GeoJsonTooltip(fields=['AVG_TIME_TO_COMPLETE'], aliases=['Avg. completion (days): '])\n",
    "folium.GeoJson(time_estimate_polygons[['geometry', 'AVG_TIME_TO_COMPLETE']].to_json(), tooltip=tooltip, name='geojson').add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Data visualization\n",
    "\n",
    "Data visualization involves creating charts, graphs, or other visualizations to communicate the insights obtained from the data. Some popular data visualization tools include Tableau, PowerBI, and ggplot.\n",
    "\n",
    "While generating visualizations is a critical yet relatively easy step in the data science process, it is not the focus of this workshop. However, we will generate a few performance indicators that bring value to business application users.\n",
    "\n",
    "Here are some queries/models that could be stored in a database view for dynamic querying. \n",
    "\n",
    "1. Upcoming work orders - Number of work orders that are in progress or in waiting\n",
    "2. Incomplete work orders (10 days slice) - Number of work orders that are in progress or in waiting grouped by slices of 10 days\n",
    "3. Average wait-time by district - Average wait time for work orders grouped by district\n",
    "4. Completed work orders by date - Number of work orders that are completed grouped by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "# Create a new database connection\n",
    "engine = create_engine('postgresql://postgres:postgres@postgres:5432/postgres')\n",
    "connection = engine.connect()\n",
    "\n",
    "print('Database connection created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and return the results as a pandas dataframe\n",
    "upcoming = pd.read_sql_query(text('''\n",
    "    SELECT * FROM orders\n",
    "    WHERE STATUS = 'En attente'\n",
    "'''), con=connection)\n",
    "\n",
    "# Render the dataframe\n",
    "upcoming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and return the results as a pandas dataframe\n",
    "slices = pd.read_sql_query(text('''\n",
    "    WITH slices as (\n",
    "        WITH RECURSIVE num_seq(n, max_val) AS (\n",
    "        SELECT 0, (\n",
    "            SELECT\n",
    "                MAX(DATE_PART('day', NOW() - order_date))\n",
    "            FROM orders\n",
    "            WHERE status = 'En attente'\n",
    "        )\n",
    "        UNION ALL\n",
    "        SELECT n + 1, max_val FROM num_seq WHERE n < max_val\n",
    "        )\n",
    "        SELECT (n / 10) * 10 as slice_start, ((n / 10) + 1) * 10 - 1 as slice_end\n",
    "        FROM num_seq\n",
    "        GROUP BY 1, 2\n",
    "    ),\n",
    "\n",
    "    _orders as (\n",
    "        SELECT\n",
    "            order_id,\n",
    "            DATE_PART('day', NOW() - order_date)::int as \"days_to_complete\"\n",
    "        FROM orders\n",
    "        WHERE status = 'En attente'\n",
    "    )\n",
    "\n",
    "    select\n",
    "        COUNT(*),\n",
    "        slice_start,\n",
    "        slice_end\n",
    "    FROM _orders\n",
    "    LEFT JOIN slices on\n",
    "        days_to_complete >= slice_start and days_to_complete <= slice_end\n",
    "    GROUP BY 2, 3\n",
    "    ORDER BY slice_start\n",
    "'''), con=connection)\n",
    "\n",
    "# Render the dataframe\n",
    "slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and return the results as a pandas dataframe\n",
    "avg_time_to_complete = pd.read_sql_query(text('''\n",
    "    SELECT \n",
    "        sector,\n",
    "        district, \n",
    "        round(AVG(DATE_PART('day', closed_date::timestamp - order_date::timestamp)::int), 2) as avg_wait_time\n",
    "    FROM orders\n",
    "    WHERE status = 'Complété'\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY avg_wait_time desc\n",
    "'''), con=connection)\n",
    "\n",
    "# Render the dataframe\n",
    "avg_time_to_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and return the results as a pandas dataframe\n",
    "completed_work_orders_per_day = pd.read_sql_query(text('''\n",
    "    SELECT\n",
    "        DATE(order_date) as order_date,\n",
    "        COUNT(*) as completed_work_orders\n",
    "    FROM orders\n",
    "    WHERE status = 'Complété'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "'''), con=connection)\n",
    "\n",
    "# Render the dataframe\n",
    "completed_work_orders_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you have learned more about the data engineering process, you can apply your new knowledge to the hackathon. Here are some tips to help you get started:\n",
    "\n",
    "1. Ensure you have real data to work with. If you are unable to find real data, you can source and collect data from public sources.\n",
    "2. Source, collect or create a deep dataset. The more data you have, the better your features and interfaces will be and the easier it will be to develop features.\n",
    "3. Ensure you understand the business problem the application is trying to solve. This will help you in identifying the features that will be most useful to the business.\n",
    "4. Try to go beyond the expected features of your application. Integrating data features that are not expected will help you stand out from the competition. Keep in mind that managers and business users will appreciate having an understanding of their business at a glance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
